{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc31b6fa",
   "metadata": {},
   "source": [
    "## 통합 네트워크 모델\n",
    "- #### 1. CNN model  \n",
    "- #### 2. LANDMARK_FCN model\n",
    "- #### 3. CNN + FCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd54b85",
   "metadata": {},
   "source": [
    "- ### 0. 데이터 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3caa314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4336548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_fer = 'C:/Users/SAMSUNG/humandata/fer2013.zip'\n",
    "# zip_object_fer = zipfile.ZipFile(file =path_fer , mode = 'r')\n",
    "# zip_object_fer.extractall('./')\n",
    "# zip_object_fer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831d7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2 ,dlib,math\n",
    "import glob\n",
    "import pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ebcb5b",
   "metadata": {},
   "source": [
    "### DATA  SET \n",
    "- ### IMAGE\n",
    "- ### LANDMARK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0cbd5f",
   "metadata": {},
   "source": [
    "#### (1) DATA SET 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0daa0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datas = 'C:/Users/SAMSUNG/humandataho/img'\n",
    "Dataset = []\n",
    "Folders = ['train', 'test']\n",
    "CATEGORIES = ['angry','fear','happy','neutral','sad','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435eaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datas = 'C:/Users/SAMSUNG/humandataho/img'\n",
    "Train_set, Test_set = [], []\n",
    "Folders = ['train', 'test']\n",
    "Categories = ['angry','fear','happy','neutral','sad','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82a0e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = dlib.shape_predictor(\"C:/Users/SAMSUNG/humandata/shape_predictor_68_face_landmarks.dat\")\n",
    "detector = dlib.get_frontal_face_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6663b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_landmarks, X_train_images, y_train = [], [], []\n",
    "X_test_landmarks, X_test_images, y_test = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd1e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLandmarks(img):\n",
    "    \n",
    "    landmarks = []\n",
    "    \n",
    "    dets = detector(img,1)\n",
    "    \n",
    "    for k, d in enumerate(dets): \n",
    "        shape = predictor(img, d) #shape: 얼굴 랜드마크 추출 \n",
    "\n",
    "        for i in range(shape.num_parts): #총 68개\n",
    "            shape_point = shape.part(i)\n",
    "            landmarks.append([shape_point.x, shape_point.y])\n",
    "    \n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d9d0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작동중.\n",
      "작동중.\n",
      "작동중.\n",
      "작동중.\n",
      "작동중.\n",
      "작동중.\n",
      "작동중.\n",
      "작동중.\n",
      "작동중.\n",
      "작동중.\n",
      "작동중.\n",
      "작동중.\n"
     ]
    }
   ],
   "source": [
    "for folder in Folders:\n",
    "    for category in Categories:\n",
    "        count = 0\n",
    "        print(\"작동중.\")\n",
    "        PATH = os.path.join(Datas, folder)\n",
    "        PATH = os.path.join(PATH, category)\n",
    "        for el in os.listdir(PATH):\n",
    "            img = cv2.imread(os.path.join(PATH,el),0)            \n",
    "            \n",
    "            try:                \n",
    "                if (category == \"angry\"):\n",
    "                     a = [1,0,0,0,0,0]\n",
    "                    #a = 1\n",
    "                if (category == \"fear\"):\n",
    "                     a = [0,1,0,0,0,0]\n",
    "                    #a = 2\n",
    "                if (category == \"happy\"):\n",
    "                     a = [0,0,1,0,0,0]\n",
    "                    #a = 3\n",
    "                if (category == \"neutral\"):\n",
    "                     a = [0,0,0,1,0,0]\n",
    "                    #a = 4\n",
    "                if (category == \"sad\"):\n",
    "                     a = [0,0,0,0,1,0]\n",
    "                    #a = 5\n",
    "                if(category == \"surprise\"):\n",
    "                     a = [0,0,0,0,0,1]\n",
    "                    #a = 6\n",
    "                \n",
    "                landmarks = getLandmarks(img)\n",
    "                \n",
    "                modified_img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "                \n",
    "#                 modified_img = cv2.resize(modified_img,(224,224))\n",
    "                \n",
    "                ## add features\n",
    "                \n",
    "                x_coordinates = [landmark[0] for landmark in landmarks]\n",
    "                y_coordinates = [landmark[1] for landmark in landmarks]\n",
    "                \n",
    "                if len(x_coordinates) == 0:\n",
    "                    continue\n",
    "                \n",
    "                left_eye_center_x = np.mean(x_coordinates[36:42])\n",
    "                left_eye_center_y = np.mean(y_coordinates[36:42])\n",
    "                \n",
    "                right_eye_center_x = np.mean(x_coordinates[42:48])\n",
    "                right_eye_center_y = np.mean(y_coordinates[42:48])\n",
    "\n",
    "                nose_center_x = np.mean(x_coordinates[27:36])\n",
    "                nose_center_y = np.mean(y_coordinates[27:36])\n",
    "\n",
    "                mouth_center_x = np.mean(x_coordinates[48:68])\n",
    "                mouth_center_y = np.mean(y_coordinates[48:68])\n",
    "                \n",
    "                # Calculate relative distances between centers\n",
    "                eye_to_nose_x = abs(left_eye_center_x - nose_center_x)\n",
    "                eye_to_nose_y = abs(left_eye_center_y - nose_center_y)\n",
    "                \n",
    "                eye_to_mouth_x = abs(left_eye_center_x - mouth_center_x)\n",
    "                eye_to_mouth_y = abs(left_eye_center_y - mouth_center_y)\n",
    "                \n",
    "                nose_to_mouth_x = abs(nose_center_x - mouth_center_x)\n",
    "                nose_to_mouth_y = abs(nose_center_y - mouth_center_y)\n",
    "                \n",
    "                landmarks.append([left_eye_center_x, left_eye_center_y])\n",
    "                landmarks.append([right_eye_center_x, right_eye_center_y])\n",
    "                landmarks.append([nose_center_x, nose_center_y])\n",
    "                landmarks.append([mouth_center_x, mouth_center_y])\n",
    "                landmarks.append([eye_to_nose_x, eye_to_nose_y])\n",
    "                landmarks.append([eye_to_mouth_x, eye_to_mouth_y])\n",
    "                landmarks.append([nose_to_mouth_x, nose_to_mouth_y])            \n",
    "                \n",
    "                \n",
    "                if(len(landmarks) != 0):\n",
    "                    if (folder == 'train'):\n",
    "                        X_train_images.append(modified_img)\n",
    "                        X_train_landmarks.append(landmarks)\n",
    "                        y_train.append(a)\n",
    "                    if (folder == 'test'):\n",
    "                        X_test_images.append(modified_img)\n",
    "                        X_test_landmarks.append(landmarks)\n",
    "                        y_test.append(a)\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "294e3c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12465, 75, 2)\n",
      "(12465, 48, 48, 3)\n",
      "(3625, 75, 2)\n",
      "(3625, 48, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train_landmarks = np.array(X_train_landmarks).astype('int8') \n",
    "X_train_images = np.array(X_train_images).astype('int8')\n",
    "y_train = np.array(y_train)\n",
    "X_test_landmarks = np.array(X_test_landmarks).astype('int8')\n",
    "X_test_images = np.array(X_test_images).astype('int8')\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(X_train_landmarks.shape)\n",
    "print(X_train_images.shape)\n",
    "print(X_test_landmarks.shape)\n",
    "print(X_test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee6fabb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_data = len(X_train_landmarks)\n",
    "num_test_data = len(X_test_landmarks)\n",
    "\n",
    "index_train = np.arange(num_train_data)\n",
    "index_test = np.arange(num_test_data)\n",
    "\n",
    "np.random.shuffle(index_train)\n",
    "np.random.shuffle(index_test)\n",
    "\n",
    "X_train_landmarks = X_train_landmarks[index_train]\n",
    "X_train_images = X_train_images[index_train]\n",
    "y_train = y_train[index_train]\n",
    "\n",
    "X_test_landmarks = X_test_landmarks[index_test]\n",
    "X_test_images = X_test_images[index_test]\n",
    "y_test = y_test[index_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f0a1c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_landmarks = np.array(X_train_landmarks).astype('float32') / 48\n",
    "X_train_images = np.array(X_train_images).astype('float32').reshape(-1,48,48,3)/ 255\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "X_test_landmarks = np.array(X_test_landmarks).astype('float32') / 48\n",
    "X_test_images = np.array(X_test_images).astype('float32').reshape(-1,48,48,3) /255\n",
    "\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c409847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# folder1 = 'C:/Users/SAMSUNG/humandataho/img/train/surprise'   #surprise\n",
    "# folder2 = 'C:/Users/SAMSUNG/humandataho/img/train/sad'  #sad\n",
    "\n",
    "# folder3 = 'C:/Users/SAMSUNG/humandataho/img/train/neutral'    #neutral\n",
    "\n",
    "# folder4 = 'C:/Users/SAMSUNG/humandataho/img/train/happy'    # happy\n",
    "\n",
    "# folder5 = 'C:/Users/SAMSUNG/humandataho/img/train/fear'    # fear\n",
    "\n",
    "# folder6 = 'C:/Users/SAMSUNG/humandataho/img/train/angry'     # angry\n",
    "\n",
    "\n",
    "# extension = '.jpg'\n",
    "\n",
    "# folder1 = glob.glob(folder1 + '/*.jpg')\n",
    "# folder2 = glob.glob(folder2 + '/*.jpg')\n",
    "# folder3 = glob.glob(folder3 + '/*.jpg')\n",
    "# folder4 = glob.glob(folder4 + '/*.jpg')\n",
    "# folder5 = glob.glob(folder5 + '/*.jpg')\n",
    "# folder6 = glob.glob(folder6 + '/*.jpg')\n",
    "# X_train = []\n",
    "# y_train = []\n",
    "# for file_name in folder1:\n",
    "#     image = cv2.imread(file_name)\n",
    "#     X_train.append(image)\n",
    "#     y_train.append(np.array([1,0,0,0,0,0]))\n",
    "\n",
    "\n",
    "# for file_name in folder2:\n",
    "#     image = cv2.imread(file_name)\n",
    "#     X_train.append(image)\n",
    "#     y_train.append(np.array([0,1,0,0,0,0]))\n",
    "\n",
    "# for file_name in folder3:\n",
    "#     image = cv2.imread(file_name)\n",
    "#     X_train.append(image)\n",
    "#     y_train.append(np.array([0,0,1,0,0,0]))\n",
    "\n",
    "# for file_name in folder4:\n",
    "#     image = cv2.imread(file_name)\n",
    "#     X_train.append(image)\n",
    "#     y_train.append(np.array([0,0,0,1,0,0]))\n",
    "\n",
    "# for file_name in folder5:\n",
    "#     image = cv2.imread(file_name)\n",
    "#     X_train.append(image)\n",
    "#     y_train.append(np.array([0,0,0,0,1,0]))\n",
    "    \n",
    "# for file_name in folder6:\n",
    "#     image = cv2.imread(file_name)\n",
    "#     X_train.append(image)\n",
    "#     y_train.append(np.array([0,0,0,0,0,1]))\n",
    "    \n",
    "# X_train = np.array(X_train)\n",
    "# y_train = np.array(y_train)\n",
    "# X_train = X_train.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3e141",
   "metadata": {},
   "source": [
    "#### (2) test_data_set 구축  , test_label 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db2b6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder10 = 'C:/Users/SAMSUNG/humandataho/img/test/surprise'     #surprise\n",
    "# folder20 = 'C:/Users/SAMSUNG/humandataho/img/test/sad'    # sad\n",
    "\n",
    "# folder30 = 'C:/Users/SAMSUNG/humandataho/img/test/neutral'   # neutral\n",
    "\n",
    "# folder40 = 'C:/Users/SAMSUNG/humandataho/img/test/happy'  # happy\n",
    "\n",
    "# folder50 = 'C:/Users/SAMSUNG/humandataho/img/test/fear' #fear\n",
    "\n",
    "# folder60 = 'C:/Users/SAMSUNG/humandataho/img/test/angry'   # angry\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# extension = '.jpg'\n",
    "\n",
    "# folder10 = glob.glob(folder10 + '/*.jpg')\n",
    "# folder20 = glob.glob(folder20 + '/*.jpg')\n",
    "# folder30 = glob.glob(folder30 + '/*.jpg')\n",
    "# folder40 = glob.glob(folder40 + '/*.jpg')\n",
    "# folder50 = glob.glob(folder50 + '/*.jpg')\n",
    "# folder60 = glob.glob(folder60 + '/*.jpg')\n",
    "# X_test = []\n",
    "# y_test = []\n",
    "# for file_name_t in folder10:\n",
    "#     image = cv2.imread(file_name)\n",
    "   \n",
    "#     X_test.append(image)\n",
    "#     y_test.append(np.array([1,0,0,0,0,0]))\n",
    "\n",
    "\n",
    "# for file_name_t in folder20:\n",
    "#     image = cv2.imread(file_name_t)\n",
    "#     X_test.append(image)\n",
    "#     y_test.append(np.array([0,1,0,0,0,0]))\n",
    "\n",
    "# for file_name_t in folder30:\n",
    "#     image = cv2.imread(file_name_t)\n",
    "#     X_test.append(image)\n",
    "#     y_test.append(np.array([0,0,1,0,0,0]))\n",
    "\n",
    "# for file_name_t in folder40:\n",
    "#     image = cv2.imread(file_name_t)\n",
    "#     X_test.append(image)\n",
    "#     y_test.append(np.array([0,0,0,1,0,0]))\n",
    "\n",
    "# for file_name_t in folder50:\n",
    "#     image = cv2.imread(file_name_t)\n",
    "#     X_test.append(image)\n",
    "#     y_test.append(np.array([0,0,0,0,1,0]))\n",
    "    \n",
    "# for file_name_t in folder60:\n",
    "#     image = cv2.imread(file_name_t)\n",
    "#     X_test.append(image)\n",
    "#     y_test.append(np.array([0,0,0,0,0,1]))\n",
    "    \n",
    "# X_test = np.array(X_test)\n",
    "# y_test = np.array(y_test)\n",
    "# X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7ecac",
   "metadata": {},
   "source": [
    "#### train , test data 인덱스 맞춰주면서 섞어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a680191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_data = len(X_train)\n",
    "\n",
    "# index_arr = np.arange(num_data)\n",
    "\n",
    "# np.random.shuffle(index_arr)\n",
    "\n",
    "# X_train = X_train[index_arr]\n",
    "\n",
    "# y_train = y_train[index_arr]\n",
    "\n",
    "# index_arr\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f22e4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_datas = len(X_test)\n",
    "\n",
    "# index_arrs = np.arange(num_datas)\n",
    "\n",
    "# np.random.shuffle(index_arrs)\n",
    "\n",
    "# X_test = X_test[index_arrs]\n",
    "\n",
    "# y_test = y_test[index_arrs]\n",
    "\n",
    "\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb125742",
   "metadata": {},
   "source": [
    "### 1. CNN 모델  - 기본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a2bba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# inputs = keras.Input(shape=(48, 48, 1))\n",
    "# x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "# x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "# x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "# x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "# x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# outputs = layers.Dense(6, activation=\"softmax\")(x)\n",
    "\n",
    "# model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63098393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60dcb0",
   "metadata": {},
   "source": [
    "### 1.1 model compile, fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82524e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=\"adam\",\n",
    "#     loss=\"categorical_crossentropy\",\n",
    "#     metrics=[\"accuracy\"])\n",
    "# history = model.fit(X_train_images, y_train, epochs=20, batch_size=64, validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec9cd9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss, test_acc = model.evaluate(X_test_images, y_test)\n",
    "# print(f\"테스트 정확도: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e43336",
   "metadata": {},
   "source": [
    "## FCN Code with add feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96aa9bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "304b18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=10,\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53e00530",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(75,2), name=\"landmarks\")\n",
    "features = layers.Dense(100, activation=\"relu\")(inputs)\n",
    "features = layers.Dense(150, activation=\"relu\")(features)\n",
    "features = layers.Dropout(0.5)(features)\n",
    "# features = layers.Dense(150, activation=\"relu\")(features)\n",
    "features = layers.Dense(100, activation=\"relu\")(features)\n",
    "features = layers.Flatten()(features)\n",
    "outputs = layers.Dense(6, activation=\"softmax\")(features)\n",
    "models = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b4ba7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " landmarks (InputLayer)      [(None, 75, 2)]           0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 75, 100)           300       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 75, 150)           15150     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 75, 150)           0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 75, 100)           15100     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 7500)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 6)                 45006     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 75,556\n",
      "Trainable params: 75,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25c4c07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01c51aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 3s 15ms/step - loss: 1.7847 - accuracy: 0.1963 - val_loss: 1.7714 - val_accuracy: 0.2475\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 2s 13ms/step - loss: 1.7072 - accuracy: 0.2863 - val_loss: 1.5721 - val_accuracy: 0.3799\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 2s 14ms/step - loss: 1.5635 - accuracy: 0.3664 - val_loss: 1.4999 - val_accuracy: 0.3967\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 2s 16ms/step - loss: 1.5028 - accuracy: 0.4028 - val_loss: 1.4700 - val_accuracy: 0.4140\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 2s 15ms/step - loss: 1.4711 - accuracy: 0.4186 - val_loss: 1.4160 - val_accuracy: 0.4533\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 2s 13ms/step - loss: 1.4544 - accuracy: 0.4188 - val_loss: 1.4541 - val_accuracy: 0.4332\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 2s 13ms/step - loss: 1.4366 - accuracy: 0.4300 - val_loss: 1.4054 - val_accuracy: 0.4553\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 2s 13ms/step - loss: 1.4149 - accuracy: 0.4430 - val_loss: 1.4326 - val_accuracy: 0.4332\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 2s 13ms/step - loss: 1.4135 - accuracy: 0.4379 - val_loss: 1.3918 - val_accuracy: 0.4597\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 2s 13ms/step - loss: 1.4118 - accuracy: 0.4413 - val_loss: 1.3867 - val_accuracy: 0.4517\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 2s 15ms/step - loss: 1.4130 - accuracy: 0.4373 - val_loss: 1.3962 - val_accuracy: 0.4617\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 2s 13ms/step - loss: 1.4000 - accuracy: 0.4443 - val_loss: 1.3893 - val_accuracy: 0.4681\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 2s 14ms/step - loss: 1.3919 - accuracy: 0.4542 - val_loss: 1.3683 - val_accuracy: 0.4777\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 2s 15ms/step - loss: 1.3913 - accuracy: 0.4552 - val_loss: 1.3809 - val_accuracy: 0.4765\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 2s 15ms/step - loss: 1.3964 - accuracy: 0.4503 - val_loss: 1.3801 - val_accuracy: 0.4657\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 2s 15ms/step - loss: 1.3866 - accuracy: 0.4516 - val_loss: 1.3877 - val_accuracy: 0.4705\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 3s 17ms/step - loss: 1.3782 - accuracy: 0.4561 - val_loss: 1.3965 - val_accuracy: 0.4565\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 3s 19ms/step - loss: 1.3801 - accuracy: 0.4581 - val_loss: 1.3759 - val_accuracy: 0.4609\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 3s 16ms/step - loss: 1.3761 - accuracy: 0.4565 - val_loss: 1.3618 - val_accuracy: 0.4813\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 3s 17ms/step - loss: 1.3676 - accuracy: 0.4634 - val_loss: 1.3960 - val_accuracy: 0.4657\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 3s 17ms/step - loss: 1.3626 - accuracy: 0.4641 - val_loss: 1.3570 - val_accuracy: 0.4902\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 3s 17ms/step - loss: 1.3650 - accuracy: 0.4647 - val_loss: 1.3515 - val_accuracy: 0.4834\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 2s 16ms/step - loss: 1.3637 - accuracy: 0.4687 - val_loss: 1.3850 - val_accuracy: 0.4721\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 3s 16ms/step - loss: 1.3643 - accuracy: 0.4569 - val_loss: 1.3602 - val_accuracy: 0.4813\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 3s 17ms/step - loss: 1.3627 - accuracy: 0.4647 - val_loss: 1.3481 - val_accuracy: 0.4749\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 2s 15ms/step - loss: 1.3520 - accuracy: 0.4683 - val_loss: 1.3496 - val_accuracy: 0.4765\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 3s 16ms/step - loss: 1.3562 - accuracy: 0.4680 - val_loss: 1.3703 - val_accuracy: 0.4545\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 2s 15ms/step - loss: 1.3523 - accuracy: 0.4666 - val_loss: 1.3504 - val_accuracy: 0.4801\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 2s 15ms/step - loss: 1.3431 - accuracy: 0.4758 - val_loss: 1.3608 - val_accuracy: 0.4765\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 2s 14ms/step - loss: 1.3440 - accuracy: 0.4689 - val_loss: 1.3819 - val_accuracy: 0.4721\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 2s 15ms/step - loss: 1.3420 - accuracy: 0.4766 - val_loss: 1.3480 - val_accuracy: 0.4765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fc13ce5b50>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.fit(\n",
    "    X_train_landmarks,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "95184b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 1s 5ms/step - loss: 1.3228 - accuracy: 0.4921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3228455781936646, 0.49213793873786926]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.evaluate(X_test_landmarks, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f066e18f",
   "metadata": {},
   "source": [
    "### 1. CNN 모델  - VGGNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "256bb7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af69256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d61b2673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 48, 48, 3)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 48, 48, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 48, 48, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 6, 6, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_layers = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "conv_layers.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce859d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in conv_layers.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ae24bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(conv_layers)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db3d9f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6)                 6150      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,246,150\n",
      "Trainable params: 531,462\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a0ad46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d03fc2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "156/156 [==============================] - 69s 438ms/step - loss: 1.7312 - accuracy: 0.2826 - val_loss: 1.6347 - val_accuracy: 0.3165\n",
      "Epoch 2/100\n",
      "156/156 [==============================] - 66s 424ms/step - loss: 1.6303 - accuracy: 0.3327 - val_loss: 1.6312 - val_accuracy: 0.3281\n",
      "Epoch 3/100\n",
      "156/156 [==============================] - 66s 424ms/step - loss: 1.6094 - accuracy: 0.3457 - val_loss: 1.6052 - val_accuracy: 0.3365\n",
      "Epoch 4/100\n",
      "156/156 [==============================] - 66s 425ms/step - loss: 1.5757 - accuracy: 0.3669 - val_loss: 1.5959 - val_accuracy: 0.3458\n",
      "Epoch 5/100\n",
      "156/156 [==============================] - 77s 496ms/step - loss: 1.5615 - accuracy: 0.3729 - val_loss: 1.5950 - val_accuracy: 0.3422\n",
      "Epoch 6/100\n",
      "156/156 [==============================] - 75s 481ms/step - loss: 1.5419 - accuracy: 0.3815 - val_loss: 1.5978 - val_accuracy: 0.3430\n",
      "Epoch 7/100\n",
      "156/156 [==============================] - 66s 424ms/step - loss: 1.5247 - accuracy: 0.3908 - val_loss: 1.5932 - val_accuracy: 0.3454\n",
      "Epoch 8/100\n",
      "156/156 [==============================] - 66s 426ms/step - loss: 1.5031 - accuracy: 0.4024 - val_loss: 1.5864 - val_accuracy: 0.3486\n",
      "Epoch 9/100\n",
      "156/156 [==============================] - 67s 429ms/step - loss: 1.4870 - accuracy: 0.4108 - val_loss: 1.5951 - val_accuracy: 0.3482\n",
      "Epoch 10/100\n",
      "156/156 [==============================] - 66s 421ms/step - loss: 1.4697 - accuracy: 0.4141 - val_loss: 1.5872 - val_accuracy: 0.3594\n",
      "Epoch 11/100\n",
      "156/156 [==============================] - 68s 434ms/step - loss: 1.4481 - accuracy: 0.4321 - val_loss: 1.5814 - val_accuracy: 0.3622\n",
      "Epoch 12/100\n",
      "156/156 [==============================] - 74s 476ms/step - loss: 1.4253 - accuracy: 0.4403 - val_loss: 1.6002 - val_accuracy: 0.3530\n",
      "Epoch 13/100\n",
      "156/156 [==============================] - 75s 483ms/step - loss: 1.3999 - accuracy: 0.4502 - val_loss: 1.6090 - val_accuracy: 0.3490\n",
      "Epoch 14/100\n",
      "156/156 [==============================] - 73s 472ms/step - loss: 1.3860 - accuracy: 0.4583 - val_loss: 1.5856 - val_accuracy: 0.3686\n",
      "Epoch 15/100\n",
      "156/156 [==============================] - 72s 460ms/step - loss: 1.3556 - accuracy: 0.4710 - val_loss: 1.5889 - val_accuracy: 0.3702\n",
      "Epoch 16/100\n",
      "156/156 [==============================] - 66s 425ms/step - loss: 1.3383 - accuracy: 0.4809 - val_loss: 1.5961 - val_accuracy: 0.3610\n",
      "Epoch 17/100\n",
      "156/156 [==============================] - 67s 432ms/step - loss: 1.3129 - accuracy: 0.4946 - val_loss: 1.6170 - val_accuracy: 0.3670\n",
      "Epoch 18/100\n",
      "156/156 [==============================] - 68s 433ms/step - loss: 1.2937 - accuracy: 0.4980 - val_loss: 1.6175 - val_accuracy: 0.3530\n",
      "Epoch 19/100\n",
      "156/156 [==============================] - 69s 440ms/step - loss: 1.2646 - accuracy: 0.5117 - val_loss: 1.6112 - val_accuracy: 0.3763\n",
      "Epoch 20/100\n",
      "156/156 [==============================] - 71s 455ms/step - loss: 1.2336 - accuracy: 0.5289 - val_loss: 1.6350 - val_accuracy: 0.3706\n",
      "Epoch 21/100\n",
      "156/156 [==============================] - 69s 440ms/step - loss: 1.2175 - accuracy: 0.5433 - val_loss: 1.6359 - val_accuracy: 0.3650\n",
      "Epoch 22/100\n",
      "156/156 [==============================] - 80s 512ms/step - loss: 1.1844 - accuracy: 0.5524 - val_loss: 1.6218 - val_accuracy: 0.3674\n",
      "Epoch 23/100\n",
      "156/156 [==============================] - 77s 493ms/step - loss: 1.1734 - accuracy: 0.5567 - val_loss: 1.6266 - val_accuracy: 0.3698\n",
      "Epoch 24/100\n",
      "156/156 [==============================] - 73s 469ms/step - loss: 1.1402 - accuracy: 0.5684 - val_loss: 1.6471 - val_accuracy: 0.3646\n",
      "Epoch 25/100\n",
      "156/156 [==============================] - 67s 430ms/step - loss: 1.1166 - accuracy: 0.5789 - val_loss: 1.6264 - val_accuracy: 0.3847\n",
      "Epoch 26/100\n",
      "156/156 [==============================] - 67s 427ms/step - loss: 1.0823 - accuracy: 0.5952 - val_loss: 1.6957 - val_accuracy: 0.3562\n",
      "Epoch 27/100\n",
      "156/156 [==============================] - 67s 428ms/step - loss: 1.0613 - accuracy: 0.6034 - val_loss: 1.6748 - val_accuracy: 0.3686\n",
      "Epoch 28/100\n",
      "156/156 [==============================] - 72s 465ms/step - loss: 1.0364 - accuracy: 0.6158 - val_loss: 1.6673 - val_accuracy: 0.3734\n",
      "Epoch 29/100\n",
      "156/156 [==============================] - 67s 432ms/step - loss: 1.0110 - accuracy: 0.6211 - val_loss: 1.6945 - val_accuracy: 0.3763\n",
      "Epoch 30/100\n",
      "156/156 [==============================] - 70s 451ms/step - loss: 0.9900 - accuracy: 0.6286 - val_loss: 1.6827 - val_accuracy: 0.3674\n",
      "Epoch 31/100\n",
      "156/156 [==============================] - 75s 478ms/step - loss: 0.9562 - accuracy: 0.6456 - val_loss: 1.7064 - val_accuracy: 0.3726\n",
      "Epoch 32/100\n",
      "156/156 [==============================] - 74s 475ms/step - loss: 0.9370 - accuracy: 0.6559 - val_loss: 1.7240 - val_accuracy: 0.3807\n",
      "Epoch 33/100\n",
      "156/156 [==============================] - 70s 447ms/step - loss: 0.9190 - accuracy: 0.6633 - val_loss: 1.7293 - val_accuracy: 0.3847\n",
      "Epoch 34/100\n",
      "156/156 [==============================] - 72s 465ms/step - loss: 0.8877 - accuracy: 0.6744 - val_loss: 1.7243 - val_accuracy: 0.3895\n",
      "Epoch 35/100\n",
      "156/156 [==============================] - 74s 472ms/step - loss: 0.8741 - accuracy: 0.6828 - val_loss: 1.7634 - val_accuracy: 0.3879\n",
      "Epoch 36/100\n",
      "156/156 [==============================] - 84s 539ms/step - loss: 0.8455 - accuracy: 0.6925 - val_loss: 1.7602 - val_accuracy: 0.3951\n",
      "Epoch 37/100\n",
      "156/156 [==============================] - 70s 447ms/step - loss: 0.8200 - accuracy: 0.6984 - val_loss: 1.7597 - val_accuracy: 0.3979\n",
      "Epoch 38/100\n",
      "156/156 [==============================] - 75s 482ms/step - loss: 0.8062 - accuracy: 0.7096 - val_loss: 1.7707 - val_accuracy: 0.3859\n",
      "Epoch 39/100\n",
      "156/156 [==============================] - 73s 471ms/step - loss: 0.7825 - accuracy: 0.7143 - val_loss: 1.7978 - val_accuracy: 0.3903\n",
      "Epoch 40/100\n",
      "156/156 [==============================] - 73s 466ms/step - loss: 0.7617 - accuracy: 0.7278 - val_loss: 1.8170 - val_accuracy: 0.4031\n",
      "Epoch 41/100\n",
      "156/156 [==============================] - 73s 466ms/step - loss: 0.7313 - accuracy: 0.7367 - val_loss: 1.8027 - val_accuracy: 0.4023\n",
      "Epoch 42/100\n",
      "156/156 [==============================] - 67s 428ms/step - loss: 0.7193 - accuracy: 0.7408 - val_loss: 1.8298 - val_accuracy: 0.3899\n",
      "Epoch 43/100\n",
      "156/156 [==============================] - 71s 457ms/step - loss: 0.6924 - accuracy: 0.7586 - val_loss: 1.8920 - val_accuracy: 0.3839\n",
      "Epoch 44/100\n",
      "156/156 [==============================] - 68s 438ms/step - loss: 0.6824 - accuracy: 0.7526 - val_loss: 1.8958 - val_accuracy: 0.3935\n",
      "Epoch 45/100\n",
      "156/156 [==============================] - 67s 428ms/step - loss: 0.6709 - accuracy: 0.7609 - val_loss: 1.8836 - val_accuracy: 0.3975\n",
      "Epoch 46/100\n",
      "156/156 [==============================] - 67s 429ms/step - loss: 0.6478 - accuracy: 0.7687 - val_loss: 1.9021 - val_accuracy: 0.4063\n",
      "Epoch 47/100\n",
      "156/156 [==============================] - 73s 466ms/step - loss: 0.6470 - accuracy: 0.7708 - val_loss: 1.9447 - val_accuracy: 0.3979\n",
      "Epoch 48/100\n",
      "156/156 [==============================] - 74s 473ms/step - loss: 0.6190 - accuracy: 0.7802 - val_loss: 1.9130 - val_accuracy: 0.3931\n",
      "Epoch 49/100\n",
      "156/156 [==============================] - 69s 445ms/step - loss: 0.6061 - accuracy: 0.7903 - val_loss: 1.9597 - val_accuracy: 0.3883\n",
      "Epoch 50/100\n",
      "156/156 [==============================] - 67s 427ms/step - loss: 0.5931 - accuracy: 0.7914 - val_loss: 1.9885 - val_accuracy: 0.3971\n",
      "Epoch 51/100\n",
      "156/156 [==============================] - 67s 427ms/step - loss: 0.5652 - accuracy: 0.7984 - val_loss: 1.9854 - val_accuracy: 0.3899\n",
      "Epoch 52/100\n",
      "156/156 [==============================] - 74s 472ms/step - loss: 0.5585 - accuracy: 0.8070 - val_loss: 2.0391 - val_accuracy: 0.3847\n",
      "Epoch 53/100\n",
      "156/156 [==============================] - 67s 432ms/step - loss: 0.5354 - accuracy: 0.8148 - val_loss: 2.0021 - val_accuracy: 0.3943\n",
      "Epoch 54/100\n",
      "156/156 [==============================] - 66s 425ms/step - loss: 0.5374 - accuracy: 0.8150 - val_loss: 2.0199 - val_accuracy: 0.4015\n",
      "Epoch 55/100\n",
      "156/156 [==============================] - 67s 429ms/step - loss: 0.5024 - accuracy: 0.8255 - val_loss: 2.0783 - val_accuracy: 0.4043\n",
      "Epoch 56/100\n",
      "156/156 [==============================] - 75s 483ms/step - loss: 0.4918 - accuracy: 0.8289 - val_loss: 2.1230 - val_accuracy: 0.3951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fc04a47eb0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.fit(X_train_images, y_train, batch_size=64, epochs=100, validation_split=0.2,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c467b287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 24s 212ms/step - loss: 2.1585 - accuracy: 0.3961\n",
      "테스트 정확도: 0.396\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_images, y_test)\n",
    "print(f\"테스트 정확도: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2938e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"VGG_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1065de",
   "metadata": {},
   "source": [
    "### 3. CNN모델 + landmark_FCN 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b06bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b75e2b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 22s 193ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn_predictions = model.predict( X_test_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6dd3d5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 3s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "fcn_predictions = models.predict(X_test_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "03bb7d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4921379310344828"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add방법 - cnn(기본) + FCN\n",
    "\n",
    "cnn_predictions = cnn_predictions * 0.0001\n",
    "fcn_predictions = fcn_predictions * 0.9999\n",
    "\n",
    "total_prediction = cnn_predictions + fcn_predictions\n",
    "\n",
    "temp_add = np.argmax(total_prediction, axis=1)\n",
    "\n",
    "temp_add = keras.utils.to_categorical(temp_add, num_classes=6).astype('int8')\n",
    "\n",
    "        \n",
    "accuracy_add = accuracy_score(y_test, temp_add)\n",
    "\n",
    "accuracy_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54a045bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VGG_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18084\\891882485.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# add방법 - cnn(VGGNET) + FCN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcnn_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGG_predictions\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfcn_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfcn_predictions\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.9999\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VGG_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# # add방법 - cnn(VGGNET) + FCN\n",
    "\n",
    "# cnn_predictions = VGG_predictions * 0.0001\n",
    "# fcn_predictions = fcn_predictions * 0.9999\n",
    "\n",
    "# total_prediction = VGG_predictions + fcn_predictions\n",
    "\n",
    "# temp_add = np.argmax(total_prediction, axis=1)\n",
    "\n",
    "# temp_add = keras.utils.to_categorical(temp_add, num_classes=6).astype('int8')\n",
    "\n",
    "        \n",
    "# accuracy_add = accuracy_score(y_test, temp_add)\n",
    "\n",
    "# accuracy_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db526511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
